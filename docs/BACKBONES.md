# DeepFilterNet4 Training Backbones

This document explains the three sequence modeling backbone architectures available for training DeepFilterNet4 on Apple Silicon using MLX.

## Overview

The **backbone** is the temporal sequence modeling component that processes audio features across time. It sits in the middle of the network architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DfNet4 Architecture                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   Audio Input                                                   â”‚
â”‚       â”‚                                                         â”‚
â”‚       â–¼                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚  ERB Encoder    â”‚  Frequency â†’ Feature extraction           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚            â”‚                                                    â”‚
â”‚            â–¼                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚    BACKBONE     â”‚  â—„â”€â”€ Temporal sequence modeling           â”‚
â”‚   â”‚  (Mamba/GRU/    â”‚      This is what we're comparing!        â”‚
â”‚   â”‚   Attention)    â”‚                                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚            â”‚                                                    â”‚
â”‚            â–¼                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚   â”‚  ERB Decoder +  â”‚  Feature â†’ Enhanced audio                 â”‚
â”‚   â”‚  Deep Filtering â”‚                                          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Performance Comparison

All benchmarks on Apple M3 Pro (18 GPU cores), batch=8, sequence=500 frames:

| Backbone | Forward | Backward | **Total** | vs Mamba | Memory |
|----------|---------|----------|-----------|----------|--------|
| **Mamba** | 575ms | 2,732ms | 3,307ms | 1.0Ã— (baseline) | Low |
| **GRU** | 77ms | 857ms | 934ms | **3.5Ã— faster** | Low |
| **Attention** | 66ms | 637ms | 703ms | **4.7Ã— faster** | Higher |

### Recommendation

| Use Case | Recommended Backbone |
|----------|---------------------|
| ğŸš€ **Fastest training** | `attention` |
| âš–ï¸ **Balance speed/memory** | `gru` |
| ğŸ“š **Research/compatibility** | `mamba` |
| ğŸ¯ **Production inference** | `gru` (streaming-friendly) |

---

## 1. Mamba (State Space Model)

```
--backbone-type mamba
```

### Architecture

Mamba is a **Selective State Space Model (S6)** that provides linear-time sequence modeling with input-dependent state transitions.

```
                    Mamba Block
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                              â”‚
    â”‚  Input x â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚                â”‚                   â”‚         â”‚
    â”‚                â–¼                   â–¼         â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚         â”‚  In Proj  â”‚       â”‚  In Proj  â”‚   â”‚
    â”‚         â”‚  (Linear) â”‚       â”‚  (Linear) â”‚   â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚               â”‚                   â”‚         â”‚
    â”‚               â–¼                   â”‚         â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚         â”‚
    â”‚         â”‚   Conv1D  â”‚             â”‚         â”‚
    â”‚         â”‚ (causal)  â”‚             â”‚         â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜             â”‚         â”‚
    â”‚               â”‚                   â”‚         â”‚
    â”‚               â–¼                   â”‚         â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚         â”‚
    â”‚         â”‚   SiLU    â”‚             â”‚         â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜             â”‚         â”‚
    â”‚               â”‚                   â”‚         â”‚
    â”‚               â–¼                   â”‚         â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚         â”‚
    â”‚    â”‚   Selective Scan     â”‚       â”‚         â”‚
    â”‚    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚         â”‚
    â”‚    â”‚   â”‚ h_t = Ä€Â·h_{t-1}  â”‚       â”‚         â”‚
    â”‚    â”‚   â”‚     + BÌ„Â·x_t   â”‚  â”‚       â”‚         â”‚
    â”‚    â”‚   â”‚ y_t = CÂ·h_t    â”‚  â”‚       â”‚         â”‚
    â”‚    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚         â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚         â”‚
    â”‚               â”‚                   â”‚         â”‚
    â”‚               â–¼                   â–¼         â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚         â”‚     Element-wise Multiply    â”‚     â”‚
    â”‚         â”‚        y Ã— SiLU(z)           â”‚     â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚                       â”‚                     â”‚
    â”‚                       â–¼                     â”‚
    â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
    â”‚                 â”‚  Out Proj â”‚               â”‚
    â”‚                 â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚
    â”‚                       â”‚                     â”‚
    â”‚  Output â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
    â”‚                                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Concepts

**State Space Model (SSM):**
```
Continuous:          Discretized (for input x):
  h'(t) = AÂ·h(t) + BÂ·x(t)    h_t = Ä€Â·h_{t-1} + BÌ„Â·x_t
  y(t)  = CÂ·h(t) + DÂ·x(t)    y_t = CÂ·h_t + DÂ·x_t
```

**Selective Mechanism:** Unlike traditional SSMs with fixed parameters, Mamba makes A, B, C **input-dependent**:
- Î” (delta): Input-dependent discretization step
- B: Input-dependent input matrix  
- C: Input-dependent output matrix

This allows the model to **selectively remember or forget** information based on content.

**Parallel Scan:** The recurrence is computed efficiently using associative scan:
```
(aâ‚, bâ‚) âŠ— (aâ‚‚, bâ‚‚) = (aâ‚‚Â·aâ‚, aâ‚‚Â·bâ‚ + bâ‚‚)
```
This reduces complexity from O(L) sequential to O(log L) parallel.

### Pros & Cons

âœ… **Pros:**
- Linear O(L) complexity in sequence length
- Constant memory during inference (state-based)
- Good at modeling long-range dependencies
- Theoretically elegant

âŒ **Cons:**
- Slow backward pass in MLX (2,732ms)
- Complex implementation
- Parallel scan not fully optimized in MLX

### Code

```python
# SqueezedMamba in df_mlx/mamba.py
self.backbone = SqueezedMamba(
    input_size=256,      # Feature dimension
    hidden_size=256,     # State dimension
    output_size=256,
    num_layers=2,        # Stacked layers
    d_state=16,          # SSM state dimension
    d_conv=4,            # Local conv kernel
    expand_factor=2,     # Inner expansion
)
```

---

## 2. GRU (Gated Recurrent Unit)

```
--backbone-type gru
```

### Architecture

GRU is a **recurrent neural network** that processes sequences step-by-step, maintaining a hidden state.

```
                    GRU Cell (single timestep)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                              â”‚
    â”‚  x_t â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚                   â”‚           â”‚         â”‚   â”‚
    â”‚  h_{t-1} â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”   â”‚   â”‚
    â”‚               â”‚   â”‚       â”‚   â”‚     â”‚   â”‚   â”‚
    â”‚               â–¼   â–¼       â–¼   â–¼     â”‚   â”‚   â”‚
    â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚   â”‚
    â”‚            â”‚   Ïƒ    â”‚  â”‚   Ïƒ    â”‚   â”‚   â”‚   â”‚
    â”‚            â”‚ (reset)â”‚  â”‚(update)â”‚   â”‚   â”‚   â”‚
    â”‚            â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â”‚   â”‚   â”‚
    â”‚                â”‚           â”‚        â”‚   â”‚   â”‚
    â”‚           r_t  â”‚      z_t  â”‚        â”‚   â”‚   â”‚
    â”‚                â”‚           â”‚        â”‚   â”‚   â”‚
    â”‚                â–¼           â”‚        â”‚   â”‚   â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚        â”‚   â”‚   â”‚
    â”‚         â”‚ r_t âŠ™ h  â”‚       â”‚        â”‚   â”‚   â”‚
    â”‚         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜       â”‚        â”‚   â”‚   â”‚
    â”‚              â”‚             â”‚        â”‚   â”‚   â”‚
    â”‚              â–¼             â”‚        â–¼   â–¼   â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚         â”‚  tanh   â”‚        â”‚   â”‚  Linear â”‚ â”‚
    â”‚         â”‚(new mem)â”‚        â”‚   â”‚ concat  â”‚ â”‚
    â”‚         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚
    â”‚              â”‚             â”‚        â”‚      â”‚
    â”‚         hÌƒ_t  â”‚             â”‚        â”‚      â”‚
    â”‚              â”‚             â”‚        â”‚      â”‚
    â”‚              â–¼             â–¼        â”‚      â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚
    â”‚    â”‚ h_t = z_tâŠ™h_{t-1} + (1-z_t)âŠ™hÌƒ_t â”‚      â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚
    â”‚                  â”‚                         â”‚
    â”‚  h_t â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
    â”‚                                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                SqueezedGRU_S Wrapper
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                            â”‚
    â”‚  Input â”€â”€â–º Linear_in â”€â”€â–º GRU â”€â”€â–º Linear_out â”€â”€â–º Output
    â”‚    â”‚       (group)              (group)         â”‚
    â”‚    â”‚                                            â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Skip Connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Equations

```
Reset gate:    r_t = Ïƒ(W_rÂ·[h_{t-1}, x_t])
Update gate:   z_t = Ïƒ(W_zÂ·[h_{t-1}, x_t])  
New memory:    hÌƒ_t = tanh(WÂ·[r_t âŠ™ h_{t-1}, x_t])
Hidden state:  h_t = z_t âŠ™ h_{t-1} + (1-z_t) âŠ™ hÌƒ_t
```

The **reset gate** (r) controls how much past information to forget.
The **update gate** (z) controls the balance between old and new information.

### Pros & Cons

âœ… **Pros:**
- Simple and well-understood
- MLX has native `nn.GRU` implementation
- Good for streaming inference (constant state size)
- 3.5Ã— faster than Mamba overall

âŒ **Cons:**
- Sequential computation (can't parallelize across time)
- Slow backward pass due to backprop-through-time (857ms)
- O(L) sequential operations in both directions

### Code

```python
# SqueezedGRU_S in df_mlx/modules.py
self.backbone = SqueezedGRU_S(
    input_size=256,
    hidden_size=256,
    output_size=256,
    num_layers=1,
    linear_groups=8,    # Grouped linear for efficiency
    gru_skip=True,      # Residual connection
)
```

---

## 3. Attention (Causal Self-Attention)

```
--backbone-type attention
```

### Architecture

Attention uses **causal self-attention** (like GPT) to model temporal dependencies with fully parallelizable operations.

```
              Multi-Head Causal Self-Attention
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                              â”‚
    â”‚  Input X â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚               â”‚         â”‚         â”‚         â”‚
    â”‚               â–¼         â–¼         â–¼         â”‚
    â”‚            â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”      â”‚
    â”‚            â”‚ W_Q â”‚   â”‚ W_K â”‚   â”‚ W_V â”‚      â”‚
    â”‚            â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜      â”‚
    â”‚               â”‚         â”‚         â”‚         â”‚
    â”‚               â–¼         â–¼         â–¼         â”‚
    â”‚              Q         K         V          â”‚
    â”‚               â”‚         â”‚         â”‚         â”‚
    â”‚               â–¼         â–¼         â”‚         â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚         â”‚
    â”‚         â”‚   Q Ã— Káµ€        â”‚       â”‚         â”‚
    â”‚         â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚       â”‚         â”‚
    â”‚         â”‚    âˆšd_k         â”‚       â”‚         â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚         â”‚
    â”‚                  â”‚                â”‚         â”‚
    â”‚                  â–¼                â”‚         â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚         â”‚
    â”‚         â”‚  Causal Mask    â”‚       â”‚         â”‚
    â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚         â”‚
    â”‚         â”‚  â”‚ 0 -âˆ -âˆ -âˆâ”‚  â”‚       â”‚         â”‚
    â”‚         â”‚  â”‚ 0  0 -âˆ -âˆâ”‚  â”‚       â”‚         â”‚
    â”‚         â”‚  â”‚ 0  0  0 -âˆâ”‚  â”‚       â”‚         â”‚
    â”‚         â”‚  â”‚ 0  0  0  0â”‚  â”‚       â”‚         â”‚
    â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚         â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚         â”‚
    â”‚                  â”‚                â”‚         â”‚
    â”‚                  â–¼                â”‚         â”‚
    â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚         â”‚
    â”‚            â”‚ Softmax  â”‚           â”‚         â”‚
    â”‚            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚         â”‚
    â”‚                 â”‚                 â”‚         â”‚
    â”‚            Attention              â”‚         â”‚
    â”‚            Weights                â”‚         â”‚
    â”‚                 â”‚                 â”‚         â”‚
    â”‚                 â–¼                 â–¼         â”‚
    â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚           â”‚   Attention Ã— V           â”‚     â”‚
    â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚                         â”‚                   â”‚
    â”‚                         â–¼                   â”‚
    â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
    â”‚                   â”‚  W_out   â”‚              â”‚
    â”‚                   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
    â”‚                        â”‚                    â”‚
    â”‚  Output â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
    â”‚                                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

            SqueezedAttention (Pre-Norm Transformer)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                  â”‚
    â”‚  Input â”€â”€â–º Linear_in â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚                        â”‚                     â”‚   â”‚
    â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
    â”‚           â”‚      Ã— num_layers       â”‚        â”‚   â”‚
    â”‚           â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚   â”‚
    â”‚           â”‚  â”‚     LayerNorm     â”‚  â”‚        â”‚   â”‚
    â”‚           â”‚  â”‚         â”‚         â”‚  â”‚        â”‚   â”‚
    â”‚           â”‚  â”‚    Attention      â”‚  â”‚        â”‚   â”‚
    â”‚           â”‚  â”‚         â”‚         â”‚  â”‚        â”‚   â”‚
    â”‚           â”‚  â”‚    + Residual â—„â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”    â”‚   â”‚
    â”‚           â”‚  â”‚         â”‚         â”‚  â”‚   â”‚    â”‚   â”‚
    â”‚           â”‚  â”‚     LayerNorm     â”‚  â”‚   â”‚    â”‚   â”‚
    â”‚           â”‚  â”‚         â”‚         â”‚  â”‚   â”‚    â”‚   â”‚
    â”‚           â”‚  â”‚       FFN         â”‚  â”‚   â”‚    â”‚   â”‚
    â”‚           â”‚  â”‚   (expandÃ—2)      â”‚  â”‚   â”‚    â”‚   â”‚
    â”‚           â”‚  â”‚         â”‚         â”‚  â”‚   â”‚    â”‚   â”‚
    â”‚           â”‚  â”‚    + Residual â—„â”€â”€â”€â”¼â”€â”€â”¼â”€â”€â”€â”˜    â”‚   â”‚
    â”‚           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚   â”‚
    â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
    â”‚                         â”‚                    â”‚   â”‚
    â”‚                         â–¼                    â”‚   â”‚
    â”‚                    Linear_out                â”‚   â”‚
    â”‚                         â”‚                    â”‚   â”‚
    â”‚                         â–¼                    â”‚   â”‚
    â”‚                    + Skip â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                         â”‚                        â”‚
    â”‚  Output â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
    â”‚                                                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Equations

**Scaled Dot-Product Attention:**
```
Attention(Q, K, V) = softmax(QKáµ€/âˆšd_k + M) Ã— V

where M is the causal mask:
      â”Œ 0    if i â‰¥ j  (can attend)
M_ij =â”‚
      â”” -âˆ   if i < j  (cannot attend to future)
```

**Why Causal?** In audio processing, we can only use past and present information, not future frames (for real-time/streaming).

### Pros & Cons

âœ… **Pros:**
- **Fully parallelizable** across time dimension
- MLX's attention is highly optimized (Metal kernels)
- **18Ã— faster backward** than GRU for the backbone alone
- **4.7Ã— faster** total training step than Mamba
- Excellent gradient flow (no vanishing gradient through time)

âŒ **Cons:**
- O(LÂ²) memory and compute in sequence length
- No persistent state (must recompute for each window)
- Not ideal for streaming inference

### Code

```python
# SqueezedAttention in df_mlx/modules.py
self.backbone = SqueezedAttention(
    input_size=256,
    hidden_size=256,
    output_size=256,
    num_layers=2,        # Pre-norm transformer layers
    num_heads=4,         # Multi-head attention
    linear_groups=8,
    gru_skip=True,       # Skip connection
)
```

---

## Training Speed Analysis

### Why is Attention Fastest?

The key insight is in the **backward pass**:

```
                    Backward Pass Comparison

    GRU (Sequential - must backprop through each timestep):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  t=500   t=499   t=498   ...   t=2    t=1    t=0   â”‚
    â”‚    â”‚       â”‚       â”‚             â”‚      â”‚      â”‚    â”‚
    â”‚    â”œâ”€â”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€â”€â”€â”€â–º ... â”œâ”€â”€â”€â”€â”€â–ºâ”œâ”€â”€â”€â”€â”€â–ºâ”‚    â”‚
    â”‚    â”‚       â”‚       â”‚             â”‚      â”‚      â”‚    â”‚
    â”‚   âˆ‚L      âˆ‚L      âˆ‚L           âˆ‚L     âˆ‚L     âˆ‚L    â”‚
    â”‚   â”€â”€      â”€â”€      â”€â”€           â”€â”€     â”€â”€     â”€â”€    â”‚
    â”‚   âˆ‚h     âˆ‚h      âˆ‚h           âˆ‚h     âˆ‚h     âˆ‚h    â”‚
    â”‚                                                     â”‚
    â”‚   Sequential: Must compute one at a time!           â”‚
    â”‚   Time: O(L) serial operations                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Attention (Parallel - all gradients computed together):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                     â”‚
    â”‚    âˆ‚L/âˆ‚Q    âˆ‚L/âˆ‚K    âˆ‚L/âˆ‚V                         â”‚
    â”‚      â”‚        â”‚        â”‚                            â”‚
    â”‚      â–¼        â–¼        â–¼                            â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
    â”‚  â”‚   Matrix multiplication   â”‚ â—„â”€â”€ GPU parallel!   â”‚
    â”‚  â”‚   (all timesteps at once) â”‚                      â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
    â”‚                                                     â”‚
    â”‚   Parallel: Compute all gradients simultaneously!   â”‚
    â”‚   Time: O(1) with enough parallelism               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Detailed Timing Breakdown

```
Component Analysis (batch=8, seq=500):

MAMBA:
â”œâ”€â”€ Forward:  575ms  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
â”‚   â””â”€â”€ Selective scan (sequential associative)
â””â”€â”€ Backward: 2732ms â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
    â””â”€â”€ Backprop through scan + input-dependent params

GRU:
â”œâ”€â”€ Forward:   77ms  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
â”‚   â””â”€â”€ MLX native GRU (optimized)
â””â”€â”€ Backward: 857ms  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
    â””â”€â”€ Backprop-through-time (500 sequential steps)

ATTENTION:
â”œâ”€â”€ Forward:   66ms  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ”‚
â”‚   â””â”€â”€ Parallel QKV projection + attention
â””â”€â”€ Backward: 637ms  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚
    â””â”€â”€ Parallel gradient computation (Metal optimized)
```

---

## Usage Examples

### Training with Each Backbone

```bash
# Fastest training (attention)
python -m df_mlx.train_dynamic \
    --cache-dir /path/to/cache \
    --backbone-type attention \
    --batch-size 32 \
    --epochs 100

# Balanced (GRU)
python -m df_mlx.train_dynamic \
    --cache-dir /path/to/cache \
    --backbone-type gru \
    --batch-size 32 \
    --epochs 100

# Original architecture (Mamba)
python -m df_mlx.train_dynamic \
    --cache-dir /path/to/cache \
    --backbone-type mamba \
    --batch-size 16 \
    --epochs 100
```

### Benchmarking

```bash
# Compare all backbones
cd DeepFilterNet
python profile_training.py
```

---

## Implementation Details

### Common Interface

All backbones share the same interface for drop-in replacement:

```python
class BackboneInterface:
    def __call__(
        self,
        x: mx.array,           # (batch, time, features)
        h: mx.array | None     # optional hidden state
    ) -> tuple[mx.array, mx.array]:
        """
        Returns:
            output: (batch, time, output_size)
            hidden: (batch, hidden_size) - last timestep
        """
```

### Memory Considerations

| Backbone | Training Memory | Inference Memory | Streaming? |
|----------|-----------------|------------------|------------|
| Mamba | O(BÃ—LÃ—DÃ—N) | O(BÃ—DÃ—N) | âœ… Yes |
| GRU | O(BÃ—LÃ—D) | O(BÃ—D) | âœ… Yes |
| Attention | O(BÃ—LÂ²Ã—H) | O(BÃ—LÂ²Ã—H) | âš ï¸ Windowed |

Where:
- B = batch size
- L = sequence length
- D = hidden dimension
- N = state dimension (Mamba)
- H = number of heads (Attention)

---

## References

1. **Mamba**: Gu, A., & Dao, T. (2023). "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
2. **GRU**: Cho, K., et al. (2014). "Learning Phrase Representations using RNN Encoder-Decoder"
3. **Attention**: Vaswani, A., et al. (2017). "Attention Is All You Need"
4. **DeepFilterNet**: SchrÃ¶ter, H., et al. (2022). "DeepFilterNet: A Low Complexity Speech Enhancement Framework"
